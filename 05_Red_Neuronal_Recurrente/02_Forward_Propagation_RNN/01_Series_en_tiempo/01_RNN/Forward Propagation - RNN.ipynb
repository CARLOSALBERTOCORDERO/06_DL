{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación paso a paso de una  red neuronal recurrente\n",
    "\n",
    "En esta actividad, implementaremos nuestra primer red neuronal recurrente.\n",
    "\n",
    "Las redes neuronales recurrentes son efectivas en el procesamiento del lenguaje natural y otras tareas secuenciales debido a que tienen \"memoria\". Las redes neuronales recurrentes pueden leer datos de entrada $x^{\\langle t \\rangle}$ (por ejemplo palabras) una a la vez, y recordar información/contexto a través de las activaciones entre las capas ocultas que se pasan de una a otra en cada al procesar cada elemento de la secuencia de entrada.   Esto permite que un red neuronal recurrente unidireccional pueda tomar información del pasado para procesar entradas posteriores. Adicionalmente, una red neuronal recurrente bidireccional puede considerar el contexto tanto del pasado como del futuro.\n",
    "\n",
    "**Notación**:\n",
    "\n",
    "- El sub-índice $i$ denota el $i$-ésimo elemento de un vector.\n",
    "\n",
    "- El super-índice $(i)$ denota un elemento asociado con el ejemplo $i^{th}$. \n",
    "\n",
    "- El super-índice $\\langle t \\rangle$ un elemento de la secuencia en el t-ésimo instante (tiempo). \n",
    "\n",
    "- El super-índice $[l]$ denota un elemento asociado con la capa $l$-ésima. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de iniciar, importemos la librería que necesitaremos para esta actividad y definamos la función `softmax()` que utilizaremos posteriormente.\n",
    "\n",
    "¿Recuardas para que hemos utilizado anteriormente la función `softmax`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from rnn_utils import *\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Forward propagation para una  red neuronal recurrente (básica)\n",
    "\n",
    "La red neuronal recurrente que implementaremos tiene la siguiente estructura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RNN.png\" style=\"width:300;height:190px;\">\n",
    "<caption><center> Figura 1: Modelo de la red neuronal recurrente </center></caption>\n",
    "\n",
    "Observe que para este ejemplo, $ T_x = T_y $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Las tareas a realizar en esta actividad son las siguientes:\n",
    "\n",
    "**Tareas**:\n",
    "1. Implementar los cálculos necesarios para un time-step de la red neuronal recurrente.\n",
    "2. Implementar un ciclo para $T_x$ time-steps con el fin de procesar todas las entradas, una a la vez.\n",
    "\n",
    "***¡Iniciemos con la actividad!***\n",
    "\n",
    "\n",
    "## 1.1 - Celda de la Red Neuronal Recurrente\n",
    "\n",
    "Una red neuronal recurrente puede verse como la repetición de una celda. Implementemos los cálculos necesarios para un time-step. La siguiente figura describe el conjunto de operaciones:\n",
    "\n",
    "<img src=\"images/rnn_step_forward.png\" style=\"width:300px;height:350px;\">\n",
    "\n",
    "\n",
    "<caption><center> Figura 2: Celda de una red neuronal recurrente. Recibe como entrada $x^{\\langle t \\rangle}$ (entrada actual) y $a^{\\langle t - 1\\rangle}$ (activación de la capa oculta previa que contenie información del pasado), y la salida $a^{\\langle t \\rangle}$ que sale hacía la siguiente celda de la red neuronal y que también predice $y^{\\langle t \\rangle}$ </center></caption>\n",
    "\n",
    "\n",
    "**Ejercicio 1**: Implementemos la celda descrita en la figura 2.\n",
    "\n",
    "**Pasos**:\n",
    "1. Calcule la activación de la capa oculta (utilizaremos la función tanh): $a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a)$.\n",
    "\n",
    "2. Utilizando la activación de la capa oculta previa $a^{\\langle t \\rangle}$, calcule la predicción $\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y)$. Para esto utilicemos la función: `softmax`.\n",
    "\n",
    "3. Guardemos en cache los valores de $a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters$.\n",
    "\n",
    "4. La salida de la celda es: $a^{\\langle t \\rangle}$ , $y^{\\langle t \\rangle}$ y el cache.\n",
    "\n",
    "Para la implementación, vectorizaremos las operaciones sobre $m$ ejemplos. Así, $x^{\\langle t \\rangle}$ tendrá la dimensión $(n_x,m)$, y $a^{\\langle t \\rangle}$ tendrá la dimensión $(n_a,m)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cell_forward(xt, a_prev, parameters):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    \n",
    "    - xt           Datos de entrada para el timestep \"t\", es un arreglo numpy de dimensiones (n_x, m)\n",
    "    \n",
    "    - a_prev       Activación previa en el timestep \"t-1\", es un arreglo numpy de dimensiones (n_a, m)\n",
    "\n",
    "    - parameters   Diccionario de python que contiene:\n",
    "                    Wax      matrix de pesos para la entrada, es un arreglo numpy de dimensiones (n_a, n_x)\n",
    "                    Waa      matrix de pesos para la activación previa, arreglo numpy de dimensiones (n_a, n_a)\n",
    "                    Wya      matrix de pesos que relaciona la cap oculta con la salida, \n",
    "                             es un arreglo numpy de dimensiones (n_y, n_a)\n",
    "                    ba       bias, arreglo numpy de dimensiones (n_a, 1)\n",
    "                    by       bias, que relaciona la capa oculta con la salida, \n",
    "                             arreglo numpy de dimensiones (n_y, 1)\n",
    "    Retorna:\n",
    "    - a_next       activación de salida, de dimensiones (n_a, m)\n",
    "    - yt_pred      predicción en el timestep \"t\", arreglo numpy de dimensiones (n_y, m)\n",
    "    - cache        una tupla de valores necesarios para el backward propagation, \n",
    "                 contiene (a_next, a_prev, xt, parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    # recuperemos los elementos del diccionario \"parameters\"\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    # calculemos la activación de salida de la celda, utilizando la fórmula descrita previament\n",
    "    a_next = None\n",
    "\n",
    "    # calculemos la salida de la celda actual, utilizando la fórmula descrita previamente\n",
    "    yt_pred = None \n",
    "    \n",
    "    # Almacena en cache los valores que necesitaremos para el backward propagation\n",
    "    cache = (None, None, None, None)\n",
    "    \n",
    "    return a_next, yt_pred, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos nuestra implementación. Para esto generemos de manera aleatorio algunos datos de entrada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)               #Definamos la semilla en 1 para poder comparar los resultados\n",
    "xt = np.random.randn(3,10)      #10 ejemplos, con 3 características cada ejemplo\n",
    "\n",
    "a_prev = np.random.randn(4,10)  # La activación tiene 4 elementos (a1, a2, a3, a4), para 10 ejemplos //\n",
    "Waa = np.random.randn(4,4)      # Generá de manera aleatoria los pesos de la matriz (4, 4)\n",
    "Wax = np.random.randn(4,3)      # Generá de manera aleatoria los pesos de la matriz (4, 3)\n",
    "Wya = np.random.randn(2,4)      # Generá de manera aleatoria los pesos de la matriz (2, 4)\n",
    "ba = np.random.randn(4,1)       # Generá de manera aleatoria el bias (4, 1)\n",
    "by = np.random.randn(2,1)       # Generá de manera aleatoria el bias (2, 1), la salida tiene dos elementos (y1, y2)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}  # Almacenamos los datos en el diccionario\n",
    "\n",
    "#Invocamos a la función cell_forward con los parámetros previamente inicializados\n",
    "a_next, yt_pred, cache = None\n",
    "\n",
    "#Verifiquemos algunos datos resultantes\n",
    "print(\"a_next.shape = \", None)\n",
    "print(\"a_next[2] = \", None)\n",
    "print(\"yt_pred.shape = \", None)\n",
    "print(\"yt_pred[0] =\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Salida esperada***\n",
    "\n",
    "`a_next.shape =  (4, 10)`\n",
    "\n",
    "`a_next[2] =  [-0.98651884  0.67677561 -0.05660542  0.94955795 -0.8008664   0.38586318\n",
    " -0.88278509  0.8682936  -0.50437243  0.38364728]`\n",
    "\n",
    "`yt_pred.shape =  (2, 10)`\n",
    "\n",
    "`yt_pred[0] = [0.03454302 0.12373132 0.04064155 0.17177199 0.03098326 0.07658893\n",
    " 0.05752428 0.26866085 0.21528314 0.395644  ]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Forward propagation\n",
    "\n",
    "Una red neuronal recurrente se puede ver como la repetición de la celda que acabamos de implementar. Si la secuencia de entrada tiene 10 elementos, entonces la celda se debe replicar 10 veces. Cada celda recibe como entrada la activación de la celda (capa) oculta previa ($a^{\\langle t-1 \\rangle}$) y el dato de entrada del time-step actual ($x^{\\langle t \\rangle}$). La celda actual, generá como salida una activación ($a^{\\langle t \\rangle}$) y una predicción ($y^{\\langle t \\rangle}$) para su time-step.\n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> Figure 3: Red neuronal recurrente. La secuencia de entrada $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  se procesa mediante $T_x$ time-steps. Las salidas de la red son $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>\n",
    "\n",
    "\n",
    "**Exercicio 2**: Implementemos el forward propagation de la red neuronal describa en la Figura 3.\n",
    "\n",
    "**Pasos**:\n",
    "1. Crear un vector inicializado en cero ($a$) que almacenará todas las activaciones calculadas por la red neuronal.\n",
    "2. Inicializar el estado de la activación $a_0$ (activación inicial)\n",
    "3. Iniciar un ciclo sobre cada time-step, el indice que imcrementará es $t$:\n",
    "    - Actualiza la siguiente activación y el cache ejecutando `cell_forward`\n",
    "    - Almacena la activación en $a$ ($t$-ésima posición) \n",
    "    - Almacena la predicción en `y`\n",
    "    - Agrega el cache a la lista de caches.\n",
    "4. Retorna $a$, $y$ y los caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def network_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implementemos el forward propagation de la red neuronal recurrente descrita en la figura 3\n",
    "    \n",
    "    Argumentos:\n",
    "    \n",
    "    x             Datos de entrada para cada time-step, de dimensiones (n_x, m, T_x).\n",
    "    \n",
    "    a0            Activación incial, de dimensiones (n_a, m)\n",
    "\n",
    "    parameters    Diccionario de python con:\n",
    "        Waa    matriz de pesos para la función de activación, arreglo numpy de dimensiones (n_a, n_a)\n",
    "        Wax    matriz de pesos para datos de entra, arreglo numpy de dimensiones (n_a, n_x)\n",
    "        Wya    matriz de pesos relacionando la activación con la salida, arreglo numpy de dimensiones (n_y, n_a)\n",
    "        ba     bias para activación, arreglo numpy de dimensiones (n_a, 1)\n",
    "        by     bias para la salida, arreglo numpy de dimensiones (n_y, 1)\n",
    "\n",
    "    Retorna:\n",
    "    a             activaciones de los time-step, arreglo numpy de dimensiones (n_a, m, T_x)\n",
    "    yp            predicciones para cada time-step, arreglo numpy de dimensiones (n_y, m, T_x)\n",
    "    caches        tupla de valores requeridos para el backward propagation. Contiene (lista de caches, x)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inicializa los \"caches\" que contendrá la lista de todos los caches\n",
    "    caches = []\n",
    "    \n",
    "    # Recupera las dimensiones de \"x\", y  parameters[\"Wya\"]\n",
    "    n_x, m, T_x = None\n",
    "    n_y, n_a = None\n",
    "        \n",
    "    # Inicializa \"a\", y \"yp\" en ceros\n",
    "    a = np.zeros([n_a,m,T_x])\n",
    "    yp = np.zeros([n_y,m,T_x])\n",
    "    \n",
    "    # Inicializa la siguiente activación: a_next \n",
    "    a_next = a0\n",
    "    \n",
    "    # Itera sobre todos los time-steps\n",
    "    for t in range(T_x):\n",
    "        \n",
    "        # Actualiza la activación, calcula la predicción, tomar el cache\n",
    "        # invocando la función cell_forward\n",
    "        \n",
    "        a_next, yp_t, cache = cell_forward(x[:,:,t], None, None)\n",
    "        \n",
    "        # Almacena el valor de la siguiente activación en a\n",
    "        a[:,:,t] = None\n",
    "        \n",
    "        # Almacena el valor de la predicción en yp \n",
    "        yp[:,:,t] = None\n",
    "        \n",
    "        # Agregar a la lista caches el cache actual \n",
    "        caches.append(None)\n",
    "        \n",
    "    \n",
    "    # Almacena los valores necesarios para el backward propagation en cache\n",
    "    caches = (None, None)\n",
    "    \n",
    "    return a, yp, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos nuestra implementación. Para esto generemos de manera aleatorio algunos datos de entrada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)               # Se define una semilla para comparar los resultados\n",
    "x = np.random.randn(3,10,4)     # Generá de manera aleatoria los datos de entrada, 4 secuencias, longitud=10, cada elemento tiene 3 características\n",
    "a0 = np.random.randn(4,10)      # Inicializa de manera aleatoria la activación inicial (4, 10)\n",
    "Waa = np.random.randn(4,4)      # Generá de manera aleatoria los pesos de la matriz (4, 4)\n",
    "Wax = np.random.randn(4,3)      # Generá de manera aleatoria los pesos de la matriz (4, 3)\n",
    "Wya = np.random.randn(2,4)      # Generá de manera aleatoria los pesos de la matriz (2, 4)\n",
    "ba = np.random.randn(4,1)       # Generá de manera aleatoria el bias (4, 1)\n",
    "by = np.random.randn(2,1)       # Generá de manera aleatoria el bias (2, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"ba\": ba, \"by\": by}\n",
    "\n",
    "# Invoquemos la función network_forward con los parámetros previamente definidos\n",
    "a, y_pred, caches = network_forward(None, None, None)\n",
    "\n",
    "# Verifiquemos algunos datos resultantes\n",
    "print(\"len(caches) = \", len(None))\n",
    "print(\"a.shape = \", None)\n",
    "print(\"a[2][1] = \", None)\n",
    "print(\"y_pred.shape = \", None)\n",
    "print(\"y_pred[1][2] =\", None)\n",
    "print(\"caches[1][1][2] =\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Salida esperada:***\n",
    "\n",
    "`len(caches) =  2`\n",
    "\n",
    "`a.shape =  (4, 10, 4)`\n",
    "\n",
    "`a[2][1] =  [-0.88357333  0.97830926 -0.92193859  0.99991081]`\n",
    "\n",
    "`y_pred.shape =  (2, 10, 4)`\n",
    "\n",
    "`y_pred[1][2] = [0.79621528 0.25637256 0.99608341 0.18712976]`\n",
    "\n",
    "`caches[1][1][2] = [ 0.12015895  0.61720311  0.30017032 -0.35224985]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta este momento hemos creado la etapa de Forward Propagation de la red neuronal recurrente. Esto funcionará lo suficientemente bien para algunas aplicaciones, pero adolece de problemas de gradiente que se desvanecen. Por lo tanto, funciona mejor cuando cada salida $y^{< t >} $ se puede estimar utilizando principalmente el contexto \"local\" (es decir, la información de las entradas $x^{\\langle t' \\rangle} $ donde $t'$ no se encuentra muy distante de $t$)."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "xxuVc",
   "launcher_item_id": "X20PE"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
