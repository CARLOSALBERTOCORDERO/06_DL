{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a7WrPI1pcRX4"
   },
   "source": [
    "# EX14 Predicción de texto utilizando redes LSTM\n",
    "\n",
    "En esta actividad, implementaremos un modelo basado en redes LSTM para la predicción de texto, utilizando una estructura: secuencia a secuencia. Recordemos que una red LSTM es un tipo de red neuronal recurrente. Una red neuronal recurrente es una red neuronal que intenta modelar comportamientos dependientes en el tiempo o secuencia, por ejemplo, el lenguaje, los precios de las acciones y la demanda de electricidad. Para entrenar el modelo, utilizaremos un conjunto de datos (texto) llamado Penn Tree Bank (`PTB`)\n",
    "[[1](https://catalog.ldc.upenn.edu/LDC99T42)]. \n",
    "\n",
    "La actividad se encuentra organizada en las siguientes etapas:\n",
    "\n",
    "- Preparación de los datos de entrenamiento, pruebas y validación\n",
    "- Estructura del modelo\n",
    "- Implementación del modelo\n",
    "- Compilación y ejecución de modelo\n",
    "- Predicción\n",
    "\n",
    "Algunas recomendaciones:\n",
    "- Realice la actividad en equipo de dos o de manera individual.\n",
    "- Lea con detenimiento la descripción de cada una de las actividades.\n",
    "- Consulte la documentación de Keras, para aquellas funciones con las que no esta familiarizado..\n",
    "- Una vez que termine con el modelo inicial, pruebe con distintos valores de los hyperparámetros.\n",
    "\n",
    "\n",
    "***¡Iniciemos con la actividad!***\n",
    "\n",
    "\n",
    "\n",
    "## Librería requeridas\n",
    "\n",
    "Importemos las librerías que utilizaremos durante la actividad:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "vtOXFXcAcRX7",
    "outputId": "124b3357-0dba-4d5e-fc44-5c99b7e6aeed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function\n",
    "import collections\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "gNUAyPAhdpU3",
    "outputId": "27c05221-d943-4cf9-97cd-8c134f87bac6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive/\n"
     ]
    }
   ],
   "source": [
    "# Mount the Google Drive to Google Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPh37u43cRX_"
   },
   "source": [
    "# 1. Preparación de los conjuntos de datos\n",
    "\n",
    "Para comenzar, descarguemos de la página del curso, el conjunto de datos `Penn Tree Bank (PTB)`. que utilizaremos como el corpus de entrenamiento y validación.\n",
    "\n",
    "Un modelo LSTM, requiere como entrada un corpus de palabras únicas asociadas a un indice entero único. Adicionalmente, el corpus necesita ser reconstituido en orden, pero en lugar de palabras de texto se utilizan los identificadores enteros en orden.\n",
    "\n",
    "Para realizar el procesamiento, implementaremos las siguientes funciones:\n",
    "\n",
    "- `read_words`, debe dividir un archivo de texto en oraciones (palabras separadas y caracteres, con el finalizador de oración `<eos>`).\n",
    "\n",
    "- `build_vocabulary`, debe identificar cada palabrá no repetida y asignarle un identificador único (valor entero)\n",
    "\n",
    "- `words_to_ids` convierte el archivo de texto original en una lista de enteros únicos, donde cada palabra se sustituye con su nuevo identificador entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpayFaKScRYA"
   },
   "outputs": [],
   "source": [
    "# Función: read_words\n",
    "def read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\", \"<eos>\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dT5uHt09cRYC"
   },
   "outputs": [],
   "source": [
    "# Función: build_vocabulary\n",
    "def build_vocabulary(filename):\n",
    "    data = read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOl20CDbcRYE"
   },
   "outputs": [],
   "source": [
    "def words_to_ids(filename, word_to_id):\n",
    "    data = read_words(filename)\n",
    "    return [word_to_id[word] for word in data if word in word_to_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Kv6AeqbcRYG"
   },
   "source": [
    "Las tres funciones anteriores nos permitirán preparar los datos originales para construir los datasets de entrenamiento, validación y de prueba, pero con cada palabra representada como un número entero en una lista. Para esto, implementemos la función: `load_data (data_path)` que recibe la ruta en donde se encuentran los archivos:\n",
    "\n",
    "- ptb_train.txt\n",
    "- ptb_valid.txt\n",
    "- ptb_test.txt\n",
    "\n",
    "La función debe hacer uso de las funciones `read_words`, `build_vocabulary`, `words_to_ids`, para construir el vocabulario, convertir las palabras a identificadores. La función debera retornar:\n",
    "\n",
    "- `train_data`: datos de entrenamiento de ids (que representan las palabras)\n",
    "- `valid_data`: datos de validación de ids (que representan las palabras)\n",
    "- `test_data`: datos de prueba con ids (que representan las palabras)\n",
    "- `vocabulary`: longitud del vocabulario\n",
    "- `reversed_dictionary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "y7xsDdcDcRYH",
    "outputId": "83923137-1566-4f00-84bb-ac22cd68a357"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./gdrive/My Drive/Colab Notebooks/ptb_corpus/ptb_train.txt\n",
      "[9970, 9971, 9972, 9974, 9975]\n",
      "aer banknote berlitz calloway centrust\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "def load_data(data_path):\n",
    "    # Ruta de los archivos de datos\n",
    "    train_path = os.path.join(data_path, 'ptb_train.txt')\n",
    "    valid_path = os.path.join(data_path, 'ptb_valid.txt')\n",
    "    test_path = os.path.join(data_path, 'ptb_test.txt')\n",
    "    print(train_path)\n",
    "\n",
    "    #Convertir el texto a una lista de enteros\n",
    "    word_to_id = build_vocabulary(train_path)\n",
    "    train_data = words_to_ids(train_path, word_to_id)\n",
    "    valid_data = words_to_ids(valid_path, word_to_id)\n",
    "    test_data = words_to_ids(test_path, word_to_id)\n",
    "    vocabulary = len(word_to_id)\n",
    "    reversed_dictionary = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
    "\n",
    "    # Imprimamos una muestra de los datos procesados\n",
    "    print(train_data[:5])\n",
    "    print(\" \".join([reversed_dictionary[x] for x in train_data[:5]]))\n",
    "    \n",
    "    print(vocabulary)\n",
    "    #print(word_to_id)\n",
    "    return train_data, valid_data, test_data, vocabulary, reversed_dictionary\n",
    "\n",
    "\n",
    "# Ejecutemos la función load_data() para construir los datasets\n",
    "data_path='./gdrive/My Drive/Colab Notebooks/ptb_corpus'    \n",
    "train_data, valid_data, test_data, vocabulary, reversed_dictionary = load_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HhfPcrCpcRYJ"
   },
   "source": [
    "## 1.1 Generación de datos (batchs)\n",
    "\n",
    "Cuando entrenamos redes neuronales, generalmente durante el entrenamiento, alimentamos los datos utilizando `batches`. Keras tiene algunas funciones útiles que pueden extraer datos de entrenamiento de manera automáticamente utilizando un objeto iterador/generador de Python. Para esto necesitamos construir el generador de manera previa e ingresarlo al modelo. En esta actividad, utilizaremos la función de Keras llamada `fit_generator`. Puede consultar la documentación [[aquí](https://keras.io/models/model/#fit_generator)].\n",
    "\n",
    "El primer argumento para `fit_generator` es la `función iterador` de Python que crearemos, y se usará para extraer lotes de datos durante el proceso de entrenamiento.  Esta función gestionará la extracción de datos, la entrada en el modelo, la ejecución de pasos del gradiente, y el registro de métricas como la precisión. El `iterador` de Python debe tener la siguiente forma:\n",
    "\n",
    "```Python\n",
    "while True:\n",
    "    #do some things to create a batch of data (x, y)\n",
    "   yield x, y\n",
    "```\n",
    "\n",
    "En nuestro caso, implementaremos una clase generadora que contendrá un método que implementa estructura anterior. Para ver ejemplos sobre el uso de generadores y yield puede consultar [[aquí](https://pythontips.com/2013/09/29/the-python-yield-keyword-explained/)].\n",
    "\n",
    "\n",
    "Los parámetros para la inicialización de la clase son:\n",
    "\n",
    "- `data`: el primer argumento son los datos que utilizará el generador. Consideremos que los datos pueden ser  entrenamiento, validación o prueba. Así que necesitaremos crear y usar múltiples instancias de la misma clase en las diversas etapas de nuestro ciclo de desarrollo de aprendizaje automático: entrenamiento, validación, pruebas. \n",
    "\n",
    "- `num_steps`: este es el número de elementos (time-steps) de la secuencia de entrada en la capa LSTM.\n",
    "\n",
    "- `batch_size`: se explica por sí mismo.\n",
    "\n",
    "- `vocabulary`: en nuestro caso es igual a 10,000. \n",
    "\n",
    "- `skip_steps`: es la cantidad de palabras que se deben saltar antes de tomar el siguiente dato de entrenamiento del batch.\n",
    "\n",
    "- `current_idx`: se inicializa en cero. Esta variable es necesaria para realizar un seguimiento de la extracción de datos a través del conjunto de datos completo: una vez que el conjunto de datos se ha consumido en el entrenamiento, debemos restablecer current_idx a cero para que el consumo de datos comience desde el inicio del conjunto de datos nuevamente. En otras palabras, es básicamente un puntero de ubicación de conjunto de datos.\n",
    "\n",
    "Implementemos el método `__init__(self, data, num_steps, batch_size, vocabulary, skip_step=5)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CH0G0LaLcRYJ"
   },
   "outputs": [],
   "source": [
    "# No ejecute esta celda de código\n",
    "\n",
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        self.current_idx = 0\n",
    "        self.skip_step = skip_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-6ID-bbcRYM"
   },
   "source": [
    "Ahora que tenemos el inicializador, es momento de implementar el método generador que será invocado durante la ejecución de `fit_generator`. Llamaremos al nuestro método `generate()` y debe realizar lo siguiente:\n",
    "\n",
    "- Crear los arrays para `x` e `y`. \n",
    "    - la dimensión de la variable `x` es: (batch_size, num_steps) \n",
    "        - el tamaño del batch.\n",
    "        - la cantidad de palabras en las que vamos a basar nuestras predicciones (longitud de la secuencia)\n",
    "        \n",
    "    - la dimensión de la variable `y` es: (batch_size, num_steps, vocabulary)\n",
    "        - el tamaño del batch\n",
    "        - la cantidad de palabras en las que vamos a basar nuestras predicciones\n",
    "        - el tamaño del vocabulario (para esta actividad 10,000)\n",
    "        \n",
    "Recordemos que la capa de salida (`y`) de nuestra red LSTM será una capa de salida con una función de activación `softmax`, que asignará una probabilidad a cada una de las 10,000 palabras posibles. La palabra con la mayor probabilidad será la palabra pronosticada; en otras palabras, la red LSTM predecirá una palabra de entre las 10,000 categorías posibles. Por lo tanto, para entrenar la red, necesitamos crear ejemplos de entrenamiento para cada palabra que tenga un 1 en la ubicación de la palabra correcta y ceros en las otras 9,999 ubicaciones (representación one hot vector: [0, 0, 0, ..., 1, 0, ..., 0, 0]). Por lo tanto, para cada palabra objetivo, debe haber un vector de longitud 10,000 con únicamente uno de los elementos del vector establecido en 1.\n",
    "\n",
    "Ahora, dentro del la estructura repetitiva `while` del generador, acorde a lo visto anteriormente:\n",
    "\n",
    "```Python\n",
    "while True:\n",
    "    #do some things to create a batch of data (x, y)\n",
    "   yield x, y\n",
    "```\n",
    "\n",
    "necesitamos generar un ejemplo de entrenamiento, para cada elemento del `batch`. En caso de que se terminen los datos, es necesario volver a iniciar en 0. Recuerde las dimensiones que tienen las variables `x` e `y`. Adicionalmente, no olvide que `y` deberá tener una representación `One hot vector`. Finalmente, el incremento de `current_idx` deberá ser el valor de `skip_step`.\n",
    "\n",
    "\n",
    "Una vez finalizado el método `generator()`, como se mencionó anteriormente, podemos configurar instancias de la misma clase para que utilicen los datos de entrenamiento y validación, de la siguiente manera:\n",
    "\n",
    "```Python\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,skip_step=num_steps)\n",
    "\n",
    "valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, vocabulary, skip_step=num_steps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHiP3pDkcRYN"
   },
   "outputs": [],
   "source": [
    "# No ejecute esta celda de código\n",
    "\n",
    "def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    #Resetear el indice\n",
    "                    self.current_idx = 0\n",
    "                    \n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # Convertir temp_y en una representación One hot vector\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uo_aq0L7cRYP"
   },
   "source": [
    "La clase KerasBatchGenerator completa debe quedar como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsJgMvDmcRYP"
   },
   "outputs": [],
   "source": [
    "# Al completar su código, ejecute esta celda para construir la clase KerasBatchGenerator\n",
    "\n",
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        self.current_idx = 0\n",
    "        self.skip_step = skip_step\n",
    "     \n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    #Resetear el indice\n",
    "                    self.current_idx = 0\n",
    "                    \n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # Convertir temp_y en una representación One hot vector\n",
    "                y[i, :, :] = to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fqQGLPNOcRYR"
   },
   "source": [
    "Probemos nuestro generador creando dos instancias, una para los datos de entrenamiento y el otro para los datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "la5B5jEzcRYS"
   },
   "outputs": [],
   "source": [
    "num_steps = 20\n",
    "batch_size = 30\n",
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step=num_steps)\n",
    "valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, vocabulary,\n",
    "                                           skip_step=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5kj36__cRYU"
   },
   "source": [
    "Ahora que los datos de entrada para nuestra modelo están configurados y listos, es hora de crear la red LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "veGTnEMucRYV"
   },
   "source": [
    "## 2. Estructura del modelo\n",
    "\n",
    "<img src=\"LSTM-model.png\" style=\"width:900;height:600px;\">\n",
    "\n",
    "<caption><center> Figura 1: Modelo de predicción de texto (estructura: secuencia a secuencia) </center></caption>\n",
    "\n",
    "La figura 1 describe la propuesta de la estructura inicial para el modelo. Veamos cada uno de los elementos de la estructura:\n",
    "\n",
    "### 2.1 Capa word embedding\n",
    "\n",
    "Recordemos que la entrada a una red LSTM no son escalares de valor único, sino secuencias (vectores de cierta longitud). Del mismo modo, todos los los pesos y bias son: matrices y vectores respectivamente. Ahora, acorde a lo visto en las sesiones anteriores ¿cómo representamos palabras para ingresarlas a una red neuronal?, la respuesta son los word embeddings. El word embedding implica tomar una palabra y encontrar una representación vectorial de esa palabra que capture algún significado semántico de la misma.  En el algoritmo `Word2Vec`, el de la palabra se cuantifica por el contexto, es decir por aquellas palabras que aparecen en oraciones cercanas a las mismas palabras.\n",
    "\n",
    "Los vectores de palabras, se pueden aprender por separado o se pueden aprender durante el entrenamiento de la red LSTM. En esta actividad, configuraremos lo que se llama una capa embedding, para convertir cada palabra en un word embedding. Para construir un capa Embedding y agregarla al modelo en Keras, utilizaremos el siguiente método:\n",
    "\n",
    "`model.add(Embedding( parameters ))`\n",
    "\n",
    "Algunos de los parámetros que requiere la capa Embedding, y que utilizaremos en esta actividad son:\n",
    "\n",
    "- input_dim:       Tamaño del vocabulario\n",
    "- input_length:    Longitud de la secuencia\n",
    "- output_dim:      Dimensión del embedding vector\n",
    "\n",
    "Para ver la documentación de la capa Embedding consulte [[aquí](https://keras.io/layers/embeddings/#embedding)].\n",
    "\n",
    "Para el diseño e implementación de redes LSTM, considere que para la capa Embedding:\n",
    "\n",
    "- La dimensión de la entrada es:\n",
    "\n",
    "    - Tensores 2D: (batch_size, sequence_length).\n",
    "\n",
    "- La dimensión de la salida es:\n",
    "\n",
    "    - Tensores 3D: (batch_size, sequence_length, output_dim).\n",
    "\n",
    "\n",
    "***Nota: la capa Embedding únicamente puede utilizarse como la mapa inicial del modelo.***\n",
    "\n",
    "La salida de la capa embedding, tendrá las dimensión: (20, 30, 500).\n",
    "\n",
    "### 2.2 Capa LSTM\n",
    "\n",
    "\n",
    "Los datos de salida de la capa Embedding, son la entrada en dos capas \"apiladas\" de celdas LSTM (de un tamaño oculto de longitud 500). En el diagrama de la figura 1, la red LSTM se muestra desenrollada. La salida de estas celdas desenrolladas tienen una dimensión: (tamaño de lote, número de pasos de tiempo, tamaño oculto).\n",
    "\n",
    "Por lo general, debe coincidir el tamaño de la salida de la capa Emedding con el número de capas ocultas en la celda LSTM. Tal vez se pregunte de dónde provienen las capas ocultas en la celda LSTM. En sesiones anteriores se presentarion las celdas LSTM de manera abstracta, y simplemente se mostró el flujo de los datos y las operaciones que se realizan sobre ellos. Sin embargo, cada unidad de activación (por ejemplo, `sigmoide` y `tanh`) en la celda, es en realidad un conjunto de unidades cuyo número es igual al tamaño de la capa oculta. Por lo tanto, cada uno de las \"unidades\" en la celda LSTM es en realidad un grupo de unidades de redes neuronal, como en cada capa de una red neuronal `FC`. Para agregar a nuestro modelo una capa LSTM utilizando Keras, utilizaremos:\n",
    "\n",
    "`model.add(LSTM( parameters ))`\n",
    "\n",
    "Algunos de los parámetros que requiere la capa LSTM, y que utilizaremos en esta actividad son:\n",
    "\n",
    "- units (int > 0). Dimensión de la salida.\n",
    "\n",
    "- return_sequences: True, si queremos que retorne toda la secuencia. False, si queremos la última salida de la sequencia.\n",
    "\n",
    "Puede ver la documentación de la capa LSTM consulte [[aquí](https://keras.io/layers/recurrent/#lstm)].\n",
    "\n",
    "### 2.3 Capa TimeDistributed\n",
    "\n",
    "Los datos de salida de la segunda capa LSTM, son la entrada a una capa de Keras llamada `TimeDistribute`. Esta capa agrega una capa independiente para cada paso de tiempo en el modelo recurrente. Entonces, por ejemplo, si tenemos 10 time-step en un modelo (es decir, trabaja con secuencias de longitud 10), una capa `TimeDistributed` que opera en una capa `Dense` produciría 10 capas densas independientes, una para cada time-step. La activación para estas capas densas está configurada para ser softmax en la capa final de nuestro modelo Keras LSTM. Para agregar a nuestro modelo una capa `TimeDistributed` con Keras, utilizaremos:\n",
    "\n",
    "`keras.layers.TimeDistributed(layer)`\n",
    "\n",
    "En nuestro caso, `layer` será una capa `Dense`. Veamos un ejemplo del uso de `TimeDistributed` para aplicar una capa densa a cada uno de 10 time-steps, de manera independientemente:\n",
    "\n",
    "```python\n",
    "model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))     # model.output_shape == (None, 10, 8)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ex9-j6jKcRYV"
   },
   "source": [
    "Finalmente, la capa de salida tiene una activación softmax, considerando para la predicción del texto que nuestro vocabulario tiene una longitud de 10,000 palabras (clases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCQzcKmWcRYW"
   },
   "source": [
    "## 3. Implementación del modelo LSTM en Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C41r_fT7cRYW"
   },
   "source": [
    "Para la implementación del modelo utilizaremos el método secuencial. Básicamente, la metodología secuencial permite apilar fácilmente capas en la red LSTM sin preocuparnos demasiado por todos los tensores (y sus dimensiones) que fluyen a través del modelo.\n",
    "\n",
    "Tomemos como base el driagrama de la figura 1 para implementar la red LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "7QhiJeUrcRYX",
    "outputId": "48dee0fe-9bfc-4f20-edb3-4f0f434fab06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 500\n",
    "use_dropout = True\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "model.add(LSTM(hidden_size, return_sequences=True))\n",
    "if use_dropout:\n",
    "    model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(vocabulary)))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNeYZksVcRYZ"
   },
   "source": [
    "## 4. Compilación y ejecución del modelo LSTM\n",
    "\n",
    "Una vez que hemos completado el modelo, ejecutemos el método de compilación. En este método, se debe especificar el tipo de pérdida que Keras debería usar para entrenar el modelo. En este caso, utilizaremos `categorical_crossentropy`. El optimizador que se utilizaremos es `Adam`. Finalmente, la métrica que utilizaremos es: `categorical_accuracy`, que nos permite ver cómo mejora la precisión durante el entrenamiento.\n",
    "\n",
    "Adicionalmente, utilizaremos un callback para un `ModelCheckPoint` para guardar el modelo después de cada época, lo que puede ser útil para cuando se está realizando un entrenamiento largo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "nUYQjgpycRYa",
    "outputId": "a8d1ba82-c92c-4eed-acb5-a18bbeda7978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 500)           5000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20, 500)           2002000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 20, 500)           2002000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 500)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 20, 10000)         5010000   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 20, 10000)         0         \n",
      "=================================================================\n",
      "Total params: 14,014,000\n",
      "Trainable params: 14,014,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/1\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "1549/1549 [==============================] - 2368s 2s/step - loss: 6.6740 - categorical_accuracy: 0.0810 - val_loss: 6.0428 - val_categorical_accuracy: 0.1391\n",
      "\n",
      "Epoch 00001: saving model to ./gdrive/My Drive/Colab Notebooks/ptb_corpus/model-01.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "#Model check pointer para almacenar el modelo cada epoca\n",
    "checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', verbose=1)\n",
    "\n",
    "#Número de epocas de entrenamiento\n",
    "num_epochs = 1\n",
    "\n",
    "model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "                    validation_data=valid_data_generator.generate(),\n",
    "                    validation_steps=len(valid_data)//(batch_size*num_steps), callbacks=[checkpointer])\n",
    "model.save(data_path + \"final_model.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6YAh9QIcRYc"
   },
   "source": [
    "## 5. Uso del modelo para realizar predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "mvOpREFqcRYd",
    "outputId": "d44f192d-8322-41cc-bec0-24902f086c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "Actual words: chairman of consolidated gold fields plc was named a nonexecutive \n",
      "Predicted words: <unk> of the the <eos> <eos> <eos> the <eos> <unk> \n",
      "Test data:\n",
      "Actual words: unable to cool the selling panic in both stocks and \n",
      "Predicted words: the to the the <unk> of <eos> the <unk> <eos> \n"
     ]
    }
   ],
   "source": [
    "# Utilicemos el modelo entrenado para realizar algunas predicciones\n",
    "\n",
    "model = load_model(data_path + \"/model-01.hdf5\")\n",
    "dummy_iters = 40\n",
    "example_training_generator = KerasBatchGenerator(train_data, num_steps, 1, vocabulary,\n",
    "                                                     skip_step=1)\n",
    "print(\"Training data:\")\n",
    "    \n",
    "for i in range(dummy_iters):\n",
    "    dummy = next(example_training_generator.generate())\n",
    "    \n",
    "num_predict = 10\n",
    "true_print_out = \"Actual words: \"\n",
    "pred_print_out = \"Predicted words: \"\n",
    "for i in range(num_predict):\n",
    "    data = next(example_training_generator.generate())\n",
    "    prediction = model.predict(data[0])\n",
    "    predict_word = np.argmax(prediction[:, num_steps-1, :])\n",
    "    true_print_out += reversed_dictionary[train_data[num_steps + dummy_iters + i]] + \" \"\n",
    "    pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "    \n",
    "print(true_print_out)\n",
    "print(pred_print_out)\n",
    "\n",
    "# test data set\n",
    "dummy_iters = 40\n",
    "example_test_generator = KerasBatchGenerator(test_data, num_steps, 1, vocabulary,\n",
    "                                             skip_step=1)\n",
    "print(\"Test data:\")\n",
    "for i in range(dummy_iters):\n",
    "    dummy = next(example_test_generator.generate())\n",
    "num_predict = 10\n",
    "true_print_out = \"Actual words: \"\n",
    "pred_print_out = \"Predicted words: \"\n",
    "for i in range(num_predict):\n",
    "    data = next(example_test_generator.generate())\n",
    "    prediction = model.predict(data[0])\n",
    "    predict_word = np.argmax(prediction[:, num_steps - 1, :])\n",
    "    true_print_out += reversed_dictionary[test_data[num_steps + dummy_iters + i]] + \" \"\n",
    "    pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "    \n",
    "print(true_print_out)\n",
    "print(pred_print_out)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ex13.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
