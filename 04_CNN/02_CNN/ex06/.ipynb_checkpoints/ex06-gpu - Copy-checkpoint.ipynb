{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex06 Red Neuronal Convolucional en TensorFlow\n",
    "\n",
    "En esta actividad, implementaremos una red neuronal convolucional utilizando TensorFlow para abordar un problema de clasificación. Se asume que ya estas familiarizado con conceptos básicos de TensorFlow como el uso de constantes, variables, tipos de datos, sesiones, placeholders y grafos de cómputo.\n",
    "\n",
    "Aprendizaje esperados:\n",
    "\n",
    "- Implementar algunas funciones de utilería que utilizaremos en el modelo ConvNet\n",
    "- Construir y entrenar una ConvNet en TensorFlow para un problema de clasificación\n",
    "\n",
    "## Uso de librerías\n",
    "\n",
    "Para comenzar con la actividad, importemos las librerías que utilizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow_core._api.v2.version' from 'c:\\\\_devtools\\\\python\\\\anaconda3\\\\envs\\\\venvgpujupyter\\\\lib\\\\site-packages\\\\tensorflow_core\\\\_api\\\\v2\\\\version\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "#import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "print(tf.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploración del dataset de entrenamiento\n",
    "\n",
    "En esta actividad, utilizaremos un conjunto de datos que contienen imágenes con señas que representan números del 0 al 5. El conjunto de datos se encuentra organizado en dos archivos: `train_signs.h5` que utilizaremos para entrenamiento y `test_signs.h5` que utilizaremso para realizar las pruebas. Ejecutemos la siguiente celda para cargar los datasets que vamos a utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/SIGNS.png\" style=\"width:600px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos visto el tipo de imágenes que contienen los `dataset`, examinemos las dimensiones de los conjuntos de datos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de continuar, normalicemos las imágenes y realicemos la conversión de las etiquetas a una representación One hot (para esto podemos utilizar la función one_hot definida en `utils.py`). Verifiquemos las dimensiones de `Y_train` y `Y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelo en TensorFlow\n",
    "\n",
    "Antes de construir el modelo, implementemos algunas funciones que utilizaremos posteriormente en el modelo de nuestra ConvNet.\n",
    "\n",
    "- init_parameters\n",
    "- forward_propagation\n",
    "- compute_cost\n",
    "\n",
    "### 2.1 Funciones base\n",
    "\n",
    "#### 2.1.1 Creación de los placeholders\n",
    "\n",
    "TensorFlow 2.X no maneja placeholders entonces no los crearemos para $X$ y la salida esperada $Y$. No debemos definir el número de ejemplos de entrenamiento por el momento. Para hacerlo, puede usar la palabra `None` como el tamaño del lote. Esto le dará la flexibilidad para definirlo más tarde. Así, $X$ debe ser de dimensiones: [None, n_H, n_W, n_C] y $Y$ debe ser de dimensiones [None, n_y].\n",
    "\n",
    "[Más información](https://www.tensorflow.org/api_docs/python/tf/placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Inicialización de parámetros\n",
    "\n",
    "Inicialicemos los filtros: $𝑊1$ y $W2$ utilizando:\n",
    "\n",
    "In tensorflow 2.0 you have a package tf.initializer with all the Keras-like initializers you need.\n",
    "\n",
    "The Xavier initializer is the same as the Glorot Uniform initializer. Thus, to create a (3,3) variable with values sampled from that initializer you can just:\n",
    "\n",
    "shape = (3,3)\n",
    "\n",
    "initializer = tf.initializers.GlorotUniform([seed = ])\n",
    "\n",
    "var = tf.Variable(initializer(shape=shape))\n",
    "\n",
    ". No necesita preocuparse por los `bias`, ya que las funciones de TensorFlow se ocupan del `bias`. Adicionalmente, considere que sólo inicializará los filtros para las funciones `conv2d` (TensorFlow inicializa las capas `FC` automáticamente). Implementemos la función `init_parameters()`. \n",
    "\n",
    "Para inicializar un parámetro 𝑊 de dimensiones [1, 2, 3, 4] en TensorFlow, utilice:\n",
    "\n",
    "```python\n",
    "W = tf.Variable(\"W\", [1,2,3,4], initializer = ...)\n",
    "```\n",
    "\n",
    "[Más información](https://www.tensorflow.org/api_docs/python/tf/get_variable).\n",
    "\n",
    "Las dimensiones para cada grupo de filtros son:\n",
    "- W1 : [4, 4, 3, 8]\n",
    "- W2 : [2, 2, 8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters():\n",
    "    \"\"\"\n",
    "    Inicializar los pesos para construir la red neuornal con TensorFlow.\n",
    "    \n",
    "    Retorna:\n",
    "    parameters --un diccionario de tensores conteniendo W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.random.set_seed(1)                              # para que sus valores aleatorios concuerden\n",
    "        \n",
    "    ### INICIA TU CÓDIGO ### \n",
    "    #W1 = tf.Variable(\"W1\",[4,4,3,8],initializer = tf.initializers.GlorotUniform(seed = 0) )\n",
    "    #W2 = tf.Variable(\"W2\", [2, 2, 8, 16], initializer=tf.initializers.GlorotUniform(seed=0))\n",
    "    initializer = tf.initializers.GlorotUniform(seed = 0)\n",
    "    W1 = tf.Variable(initializer(shape=[4,4,3,8]))\n",
    "    W2 = tf.Variable(initializer(shape=[2, 2, 8, 16]))\n",
    "    ### FIN DE TU CÓDIGO ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [-0.05346771  0.18349849 -0.01215445  0.00138046  0.0012947  -0.02904211\n",
      " -0.11260509 -0.143055  ]\n",
      "W2 = [-0.1713624   0.09527719 -0.0744766  -0.02245569  0.24450928 -0.06879854\n",
      "  0.21546292 -0.08803296 -0.16513646 -0.19527972 -0.22957063  0.15745944\n",
      "  0.13090086 -0.12304181 -0.05287278  0.03434092]\n"
     ]
    }
   ],
   "source": [
    "parameters = init_parameters()\n",
    "\n",
    "print(f\"W1 = {parameters['W1'][1,1,1]}\")\n",
    "print(f\"W2 = {parameters['W2'][1,1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Forward propagation\n",
    "\n",
    "TensorFlow provee un conjuto de funciones para realizar las convoluciones (no es necesario implementarlas desde cero).\n",
    "\n",
    "- tf.nn.conv2d(X,W, strides = [1,s,s,1], padding = 'SAME')\n",
    "\n",
    "    - Dada una entrada `X` y un grupo de filtros `W`, la función `conv2` aplica la convolución de `W` sobre `X`. El tercer parámetro `[1, s, s, 1]` representa el `stride` para cada dimensión de la entrada `(m, n_H_prev, n_W_prev, n_C_prev)`. [[documentación]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/conv2d)\n",
    "\n",
    "\n",
    "- tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME')\n",
    "    - Data una entrada `A`, la función max_pool utiliza una ventana de dimensiones `(f, f)` y `stride` de tamaño `(s, s)` para realizar el `max pooling` sobre cada ventana. [[documentación]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/max_pool)\n",
    "    ksize[channel/s, x, y, outpus]\n",
    "    \n",
    "    \n",
    "- tf.nn.relu(Z)\n",
    "    - Calcula la función `ReLU` a los elementos de `Z` (puede tener cualquier forma). [[documentación]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/relu)\n",
    "\n",
    "\n",
    "- tf.contrib.layers.flatten(A)\n",
    "    - Dada una entrada A, esta función aplana cada ejemplo en un vector 1D mientras mantiene el tamaño del lote. Devuelve un tensor aplanado con forma `[batch_size, k]`. [[documentación]](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten)\n",
    "\n",
    "\n",
    "- tf.contrib.layers.fully_connected(F, num_outputs)\n",
    "    - Dada una entrada aplanada F, devuelve la salida calculada usando una capa completamente conectada. [[documentación]](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected)\n",
    "\n",
    "$\\textbf{Nota}$: La función `tf.contrib.layers.fully_connected`, inicializa automáticamente los pesos en el grafo y continúa entrenándolos durante el entrenamiento del modelo. Por lo tanto, no necesita inicializar esos pesos al inicializar los parámetros.\n",
    "\n",
    "Utilizando las funciones descritas anteriormente, implementemos la función `forward_propagation`  para construir el siguiente modelo: \n",
    "\n",
    "\n",
    "`CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`\n",
    "\n",
    "\n",
    "Para este ejercicio, utilicemos los siguientes parámetros:\n",
    "\n",
    "- $\\textbf{CONV2D}$: stride=1, padding=\"SAME\"\n",
    "\n",
    "- $\\textbf{RELU}$\n",
    "\n",
    "- $\\textbf{MAXPOOL}$: filtro de 8x8, stride = 8, padding = \"SAME\"\n",
    "\n",
    "- $\\textbf{CONV2D}$: stride=1, padding = \"SAME\"\n",
    "\n",
    "- $\\textbf{RELU}$\n",
    "\n",
    "- $\\textbf{MAXPOOL}$: filtro de 4x4, stride = 4, padding = \"SAME\"\n",
    "\n",
    "- $\\textbf{FLATTEN}$\n",
    "\n",
    "- $\\textbf{FULLYCONNECTED (FC) layer}$.\n",
    "\n",
    "$\\text{Nota}$: la capa FC dará como resultado 6 neuronas en la capa de salida, que luego pasarán a un función `softmax`. En TensorFlow, las funciones `softmax` y `cost` se agrupan en una sola función, que se invocará posteriormente al calcular el costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, nClasses):\n",
    "    \"\"\"\n",
    "    Implementación de forward propagation para el modelo:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Parámetros:\n",
    "    X             placeholder del dataset de entrada, de dimensiones (input size, number of examples)\n",
    "    parameters    diccionario de python que contiene los parámetros W1, W2\n",
    "                  recordemos que las dimensiones se definen en la inicialización de parámetros.\n",
    "\n",
    "    Retorna:\n",
    "    Z3            la salida de la última unidad lineal\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recupera los parámetros del diccionario \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    ### INICIA TU CÓDIGO ###\n",
    "    \n",
    "    # CONV2D: filtros = W1, stride = 1, padding = 'SAME'\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1] , padding = 'SAME')\n",
    "    \n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    \n",
    "    # MAXPOOL: filtro = 8x8, stride = 8, padding = 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
    "    \n",
    "    # CONV2D: filtros = W2, stride=1, padding='SAME'\n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    \n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    \n",
    "    # MAXPOOL: filtro = 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "    \n",
    "    # FLATTEN\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    \n",
    "    # Capa FC con \"activation_fn=None\" \n",
    "    Z3 = tf.contrib.layers.fully_connected(P2 , nClasses, activation_fn=None )\n",
    "    \n",
    "    ### FIN DE TU CODIGO ###\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-abf9aac43612>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mZ3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#a = session.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Z3 = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mZ3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-962a99732383>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(X, parameters, nClasses)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# CONV2D: filtros = W1, stride = 1, padding = 'SAME'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mZ1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# RELU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_devtools\\python\\anaconda3\\envs\\venvgpujupyter\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_v2\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1911\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m                 \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1913\u001b[1;33m                 name=name)\n\u001b[0m\u001b[0;32m   1914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_devtools\\python\\anaconda3\\envs\\venvgpujupyter\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[0;32m   2008\u001b[0m                            \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m                            \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m                            name=name)\n\u001b[0m\u001b[0;32m   2011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_devtools\\python\\anaconda3\\envs\\venvgpujupyter\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1037\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_devtools\\python\\anaconda3\\envs\\venvgpujupyter\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "#X, Y = create_placeholder(64, 64, 3, 6)\n",
    "parameters = init_parameters()\n",
    "X = tf.constant(np.random.randn(2,64,64,3), tf.float32)\n",
    "Z3 = forward_propagation(X, parameters, 6)\n",
    "#a = session.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n",
    "print(\"Z3 = \" + Z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Calcular de costo\n",
    "\n",
    "Implementemos el cálculo de la función de costo. Para esto, utilizaremos las siguiente funciones que proporciona TensorFlow y que nos permiten evitar una implementació desde cero.\n",
    " \n",
    "- tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y)\n",
    "    - Esta función calcula la función de activación softmax y la pérdida resultante.  [[Documentación]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "\n",
    "\n",
    "- tf.reduce_mean\n",
    "    - Calcula la media de elementos a través de las dimensiones de un tensor. Utilice esta función para sumar las pérdidas sobre todos los ejemplos y así obtener el costo total. [[Documentación]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/math/reduce_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(Z, Y):\n",
    "    \"\"\"\n",
    "    Calcular el costo\n",
    "    \n",
    "    Parámetros:\n",
    "    Z     salida del forward propagation (salida de la última unidad de activación lineal), de dimensiones (6, numero de ejemplos)\n",
    "    Y     placeholder de vector con etiquetas reales, tiene la misma dimensión que Z\n",
    "    \n",
    "    Retorna:\n",
    "    cost Tensor del costo\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIO DE TU CÓDIGO ###\n",
    "    clogs = None\n",
    "    cost = None\n",
    "    ### FIN DE TU CÓDIGO ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholder(64, 64, 3, 6)\n",
    "    parameters = init_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = cost_function(Z3, Y)\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    a = session.run(cost, {X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)})\n",
    "    print(\"cost = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Integración del modelo ConvNet\n",
    "\n",
    "Finalmente, integremos las funciones que implementamos anteriormente para construir un modelo. Posteriormente, entrenaremos el conjunto de datos `train_signs.h5` utilizando mini lotes. En el archivo `utils.py` se encuentra implementada la función `random_mini_batches()`. Recuerde que esta función devuelve una lista de mini lotes.\n",
    "\n",
    "El modelo debería:\n",
    "\n",
    "- Crear placeholders\n",
    "- Inicializar parámetros\n",
    "- Realizar el forward propagation\n",
    "- Calcular el costo\n",
    "- Crear un optimizador\n",
    "\n",
    "Finalmente, creará una sesión y ejecutará un bucle for para num_epochs, obtendrá los mini lotes y luego, para cada mini lote, optimizará la función. [[doc para inicialización las variables]](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)\n",
    "\n",
    "Completemos la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.006, num_epochs = 100, minibatch_size = 64):\n",
    "    \"\"\"\n",
    "    Recordemos la estructura de la ConvNet:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Parámetros: \n",
    "    X_train          dataset de entrenamiento, dimensiones (None, 64, 64, 3)\n",
    "    Y_train          etiquetas reales del dataset de entrenamiento, dimensiones (None, n_y = 6)\n",
    "    X_test           dataset de pruebas, dimensiones (None, 64, 64, 3)\n",
    "    Y_test           etiquetas reales del dataset de pruebas, dimensiones (None, n_y = 6)\n",
    "    learning_rate    tasa de aprendizaje del optimizador\n",
    "    num_epochs       número de epocas del ciclo del optimizador\n",
    "    minibatch_size   tamaño del mini batch \n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                   # permite ejecutar el modelo sin sobre escribir las variables tf\n",
    "    tf.set_random_seed(1)                       # para mantener los resultados consistentes tensorflow\n",
    "    seed = 3                                    # para mantener los resultados consistentes en numpy \n",
    "    \n",
    "    (m, n_H0, n_W0, n_C0) = None       # identificar las dimensiones del dataset de entrenamiento\n",
    "    n_y = None                      # identificar el número de etiquetas\n",
    "    costs = []                                  # Para mantener un registro de los costos\n",
    "    \n",
    "    # Crear placeholders con las dimensiones correctas\n",
    "    ### INICIA TU CÓDIGO ### \n",
    "    X, Y = None\n",
    "    ### FINALIZA TU CÓDIGO ###\n",
    "\n",
    "    # Inicializar los parámetros\n",
    "    ### INICIA TU CÓDIGO ### (1 line)\n",
    "    parameters = None\n",
    "    ### FINALIZA TU CÓDIGO ###\n",
    "    \n",
    "    # Forward propagation: crear el forward propagation en el grafo de cómputo de TensorFlow\n",
    "    ### INICIA TU CÓDIGO ### (1 line)\n",
    "    Z3 = None\n",
    "    ### FINALIZA TU CÓDIGO ###\n",
    "    \n",
    "    # Función de costo: agrega la función de costo al grafo de cómputo\n",
    "    ### INICIA TU CÓDIGO ### (1 line)\n",
    "    cost = None\n",
    "    ### FINALIZA TU CÓDIGO ###\n",
    "    \n",
    "    # Backpropagation: definir el optimizador en TensorFlow. Utilicemos un optimizador AdamOptimizer\n",
    "    # para minimizar el costo\n",
    "    \n",
    "    ### INICIA TU CÓDIGO ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    ### FINALIZA TU CÓDIGO ###\n",
    "    \n",
    "    # Inicializar globalmente todas las variables\n",
    "    init = None\n",
    "     \n",
    "    # Iniciar la sesión para evaluar el grafo de TensorFlow\n",
    "    with tf.Session() as session:\n",
    "        \n",
    "        # Ejecutar la inicialización\n",
    "        session.run(None)\n",
    "        \n",
    "        # Ciclo de entrenamiento\n",
    "        for epoch in range(num_epochs):\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed) #Definida en utils.py\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # Selecciona un minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: Ejecución del grafo sobre un minibatch.\n",
    "                # Ejecutar la sesión para ejecutar el optimizador y la función de costo, t\n",
    "                # el feed_dict debe contener un minibatch (X,Y).\n",
    "                ### INICIA TU CÓDIGO ### \n",
    "                _ , temp_cost = session.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "                ### FINALIZA TU CÓDIGO ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "            # Imprimir el costo\n",
    "            if epoch % 10 == 0:\n",
    "                print (f\"Costo despues de la epoca {epoch}: {None}\")\n",
    "            if epoch % 1 == 0:\n",
    "                costs.append(None)\n",
    "        \n",
    "        \n",
    "        # graficar el costo\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('Costo')\n",
    "        plt.xlabel('Epocas')\n",
    "        plt.title(\"Tasa de aprendizaje:\" + str(None))\n",
    "        plt.show()\n",
    "\n",
    "        # Calcular las predicciones correctas\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calcular la exactitud sobre el conjunto de pruebas\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Exactitud (Training dataset):\", train_accuracy)\n",
    "        print(\"Exactitud (Test datset):\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
