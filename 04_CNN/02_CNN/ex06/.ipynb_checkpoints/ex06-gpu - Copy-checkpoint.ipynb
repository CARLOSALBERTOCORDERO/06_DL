{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex06 Red Neuronal Convolucional en TensorFlow\n",
    "\n",
    "En esta actividad, implementaremos una red neuronal convolucional utilizando TensorFlow para abordar un problema de clasificaci贸n. Se asume que ya estas familiarizado con conceptos b谩sicos de TensorFlow como el uso de constantes, variables, tipos de datos, sesiones, placeholders y grafos de c贸mputo.\n",
    "\n",
    "Aprendizaje esperados:\n",
    "\n",
    "- Implementar algunas funciones de utiler铆a que utilizaremos en el modelo ConvNet\n",
    "- Construir y entrenar una ConvNet en TensorFlow para un problema de clasificaci贸n\n",
    "\n",
    "## Uso de librer铆as\n",
    "\n",
    "Para comenzar con la actividad, importemos las librer铆as que utilizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow_core._api.v2.version' from 'c:\\\\_devtools\\\\python\\\\anaconda3\\\\envs\\\\venvgpujupyter\\\\lib\\\\site-packages\\\\tensorflow_core\\\\_api\\\\v2\\\\version\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "#import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "print(tf.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploraci贸n del dataset de entrenamiento\n",
    "\n",
    "En esta actividad, utilizaremos un conjunto de datos que contienen im谩genes con se帽as que representan n煤meros del 0 al 5. El conjunto de datos se encuentra organizado en dos archivos: `train_signs.h5` que utilizaremos para entrenamiento y `test_signs.h5` que utilizaremso para realizar las pruebas. Ejecutemos la siguiente celda para cargar los datasets que vamos a utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/SIGNS.png\" style=\"width:600px;height:250px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos visto el tipo de im谩genes que contienen los `dataset`, examinemos las dimensiones de los conjuntos de datos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de continuar, normalicemos las im谩genes y realicemos la conversi贸n de las etiquetas a una representaci贸n One hot (para esto podemos utilizar la funci贸n one_hot definida en `utils.py`). Verifiquemos las dimensiones de `Y_train` y `Y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelo en TensorFlow\n",
    "\n",
    "Antes de construir el modelo, implementemos algunas funciones que utilizaremos posteriormente en el modelo de nuestra ConvNet.\n",
    "\n",
    "- init_parameters\n",
    "- forward_propagation\n",
    "- compute_cost\n",
    "\n",
    "### 2.1 Funciones base\n",
    "\n",
    "#### 2.1.1 Creaci贸n de los placeholders\n",
    "\n",
    "TensorFlow 2.X no maneja placeholders entonces no los crearemos para $X$ y la salida esperada $Y$. No debemos definir el n煤mero de ejemplos de entrenamiento por el momento. Para hacerlo, puede usar la palabra `None` como el tama帽o del lote. Esto le dar谩 la flexibilidad para definirlo m谩s tarde. As铆, $X$ debe ser de dimensiones: [None, n_H, n_W, n_C] y $Y$ debe ser de dimensiones [None, n_y].\n",
    "\n",
    "[M谩s informaci贸n](https://www.tensorflow.org/api_docs/python/tf/placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Inicializaci贸n de par谩metros\n",
    "\n",
    "Inicialicemos los filtros: $1$ y $W2$ utilizando:\n",
    "\n",
    "In tensorflow 2.0 you have a package tf.initializer with all the Keras-like initializers you need.\n",
    "\n",
    "The Xavier initializer is the same as the Glorot Uniform initializer. Thus, to create a (3,3) variable with values sampled from that initializer you can just:\n",
    "\n",
    "shape = (3,3)\n",
    "\n",
    "initializer = tf.initializers.GlorotUniform([seed = ])\n",
    "\n",
    "var = tf.Variable(initializer(shape=shape))\n",
    "\n",
    ". No necesita preocuparse por los `bias`, ya que las funciones de TensorFlow se ocupan del `bias`. Adicionalmente, considere que s贸lo inicializar谩 los filtros para las funciones `conv2d` (TensorFlow inicializa las capas `FC` autom谩ticamente). Implementemos la funci贸n `init_parameters()`. \n",
    "\n",
    "Para inicializar un par谩metro  de dimensiones [1, 2, 3, 4] en TensorFlow, utilice:\n",
    "\n",
    "```python\n",
    "W = tf.Variable(\"W\", [1,2,3,4], initializer = ...)\n",
    "```\n",
    "\n",
    "[M谩s informaci贸n](https://www.tensorflow.org/api_docs/python/tf/get_variable).\n",
    "\n",
    "Las dimensiones para cada grupo de filtros son:\n",
    "- W1 : [4, 4, 3, 8]\n",
    "- W2 : [2, 2, 8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters():\n",
    "    \"\"\"\n",
    "    Inicializar los pesos para construir la red neuornal con TensorFlow.\n",
    "    \n",
    "    Retorna:\n",
    "    parameters --un diccionario de tensores conteniendo W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.random.set_seed(1)                              # para que sus valores aleatorios concuerden\n",
    "        \n",
    "    ### INICIA TU CDIGO ### \n",
    "    #W1 = tf.Variable(\"W1\",[4,4,3,8],initializer = tf.initializers.GlorotUniform(seed = 0) )\n",
    "    #W2 = tf.Variable(\"W2\", [2, 2, 8, 16], initializer=tf.initializers.GlorotUniform(seed=0))\n",
    "    initializer = tf.initializers.GlorotUniform(seed = 0)\n",
    "    W1 = tf.Variable(initializer(shape=[4,4,3,8]))\n",
    "    W2 = tf.Variable(initializer(shape=[2, 2, 8, 16]))\n",
    "    ### FIN DE TU CDIGO ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [-0.05346771  0.18349849 -0.01215445  0.00138046  0.0012947  -0.02904211\n",
      " -0.11260509 -0.143055  ]\n",
      "W2 = [-0.1713624   0.09527719 -0.0744766  -0.02245569  0.24450928 -0.06879854\n",
      "  0.21546292 -0.08803296 -0.16513646 -0.19527972 -0.22957063  0.15745944\n",
      "  0.13090086 -0.12304181 -0.05287278  0.03434092]\n"
     ]
    }
   ],
   "source": [
    "parameters = init_parameters()\n",
    "\n",
    "print(f\"W1 = {parameters['W1'][1,1,1]}\")\n",
    "print(f\"W2 = {parameters['W2'][1,1,1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Forward propagation\n",
    "\n",
    "TensorFlow provee un conjuto de funciones para realizar las convoluciones (no es necesario implementarlas desde cero).\n",
    "\n",
    "- tf.nn.conv2d(X,W, strides = [1,s,s,1], padding = 'SAME')\n",
    "\n",
    "    - Dada una entrada `X` y un grupo de filtros `W`, la funci贸n `conv2` aplica la convoluci贸n de `W` sobre `X`. El tercer par谩metro `[1, s, s, 1]` representa el `stride` para cada dimensi贸n de la entrada `(m, n_H_prev, n_W_prev, n_C_prev)`. [[documentaci贸n]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/conv2d)\n",
    "\n",
    "\n",
    "- tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME')\n",
    "    - Data una entrada `A`, la funci贸n max_pool utiliza una ventana de dimensiones `(f, f)` y `stride` de tama帽o `(s, s)` para realizar el `max pooling` sobre cada ventana. [[documentaci贸n]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/max_pool)\n",
    "    ksize[channel/s, x, y, outpus]\n",
    "    \n",
    "    \n",
    "- tf.nn.relu(Z)\n",
    "    - Calcula la funci贸n `ReLU` a los elementos de `Z` (puede tener cualquier forma). [[documentaci贸n]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/relu)\n",
    "\n",
    "\n",
    "- tf.contrib.layers.flatten(A)\n",
    "    - Dada una entrada A, esta funci贸n aplana cada ejemplo en un vector 1D mientras mantiene el tama帽o del lote. Devuelve un tensor aplanado con forma `[batch_size, k]`. [[documentaci贸n]](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten)\n",
    "\n",
    "\n",
    "- tf.contrib.layers.fully_connected(F, num_outputs)\n",
    "    - Dada una entrada aplanada F, devuelve la salida calculada usando una capa completamente conectada. [[documentaci贸n]](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected)\n",
    "\n",
    "$\\textbf{Nota}$: La funci贸n `tf.contrib.layers.fully_connected`, inicializa autom谩ticamente los pesos en el grafo y contin煤a entren谩ndolos durante el entrenamiento del modelo. Por lo tanto, no necesita inicializar esos pesos al inicializar los par谩metros.\n",
    "\n",
    "Utilizando las funciones descritas anteriormente, implementemos la funci贸n `forward_propagation`  para construir el siguiente modelo: \n",
    "\n",
    "\n",
    "`CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`\n",
    "\n",
    "\n",
    "Para este ejercicio, utilicemos los siguientes par谩metros:\n",
    "\n",
    "- $\\textbf{CONV2D}$: stride=1, padding=\"SAME\"\n",
    "\n",
    "- $\\textbf{RELU}$\n",
    "\n",
    "- $\\textbf{MAXPOOL}$: filtro de 8x8, stride = 8, padding = \"SAME\"\n",
    "\n",
    "- $\\textbf{CONV2D}$: stride=1, padding = \"SAME\"\n",
    "\n",
    "- $\\textbf{RELU}$\n",
    "\n",
    "- $\\textbf{MAXPOOL}$: filtro de 4x4, stride = 4, padding = \"SAME\"\n",
    "\n",
    "- $\\textbf{FLATTEN}$\n",
    "\n",
    "- $\\textbf{FULLYCONNECTED (FC) layer}$.\n",
    "\n",
    "$\\text{Nota}$: la capa FC dar谩 como resultado 6 neuronas en la capa de salida, que luego pasar谩n a un funci贸n `softmax`. En TensorFlow, las funciones `softmax` y `cost` se agrupan en una sola funci贸n, que se invocar谩 posteriormente al calcular el costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, nClasses):\n",
    "    \"\"\"\n",
    "    Implementaci贸n de forward propagation para el modelo:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Par谩metros:\n",
    "    X             placeholder del dataset de entrada, de dimensiones (input size, number of examples)\n",
    "    parameters    diccionario de python que contiene los par谩metros W1, W2\n",
    "                  recordemos que las dimensiones se definen en la inicializaci贸n de par谩metros.\n",
    "\n",
    "    Retorna:\n",
    "    Z3            la salida de la 煤ltima unidad lineal\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recupera los par谩metros del diccionario \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    ### INICIA TU CDIGO ###\n",
    "    \n",
    "    # CONV2D: filtros = W1, stride = 1, padding = 'SAME'\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1] , padding = 'SAME')\n",
    "    \n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    \n",
    "    # MAXPOOL: filtro = 8x8, stride = 8, padding = 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
    "    \n",
    "    # CONV2D: filtros = W2, stride=1, padding='SAME'\n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    \n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    \n",
    "    # MAXPOOL: filtro = 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "    \n",
    "    # FLATTEN\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    \n",
    "    # Capa FC con \"activation_fn=None\" \n",
    "    Z3 = tf.contrib.layers.fully_connected(P2 , nClasses, activation_fn=None )\n",
    "    \n",
    "    ### FIN DE TU CODIGO ###\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-abf9aac43612>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mZ3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#a = session.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Z3 = \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mZ3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-962a99732383>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(X, parameters, nClasses)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m# CONV2D: filtros = W1, stride = 1, padding = 'SAME'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mZ1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# RELU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_devtools\\python\\anaconda3\\envs\\venvgpujupyter\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d_v2\u001b[1;34m(input, filters, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1911\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m                 \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1913\u001b[1;33m                 name=name)\n\u001b[0m\u001b[0;32m   1914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_devtools\\python\\anaconda3\\envs\\venvgpujupyter\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[0;32m   2008\u001b[0m                            \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m                            \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m                            name=name)\n\u001b[0m\u001b[0;32m   2011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_devtools\\python\\anaconda3\\envs\\venvgpujupyter\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1037\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\_devtools\\python\\anaconda3\\envs\\venvgpujupyter\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "#X, Y = create_placeholder(64, 64, 3, 6)\n",
    "parameters = init_parameters()\n",
    "X = tf.constant(np.random.randn(2,64,64,3), tf.float32)\n",
    "Z3 = forward_propagation(X, parameters, 6)\n",
    "#a = session.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n",
    "print(\"Z3 = \" + Z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Calcular de costo\n",
    "\n",
    "Implementemos el c谩lculo de la funci贸n de costo. Para esto, utilizaremos las siguiente funciones que proporciona TensorFlow y que nos permiten evitar una implementaci贸 desde cero.\n",
    " \n",
    "- tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y)\n",
    "    - Esta funci贸n calcula la funci贸n de activaci贸n softmax y la p茅rdida resultante.  [[Documentaci贸n]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "\n",
    "\n",
    "- tf.reduce_mean\n",
    "    - Calcula la media de elementos a trav茅s de las dimensiones de un tensor. Utilice esta funci贸n para sumar las p茅rdidas sobre todos los ejemplos y as铆 obtener el costo total. [[Documentaci贸n]](https://www.tensorflow.org/versions/r1.14/api_docs/python/tf/math/reduce_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(Z, Y):\n",
    "    \"\"\"\n",
    "    Calcular el costo\n",
    "    \n",
    "    Par谩metros:\n",
    "    Z     salida del forward propagation (salida de la 煤ltima unidad de activaci贸n lineal), de dimensiones (6, numero de ejemplos)\n",
    "    Y     placeholder de vector con etiquetas reales, tiene la misma dimensi贸n que Z\n",
    "    \n",
    "    Retorna:\n",
    "    cost Tensor del costo\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INICIO DE TU CDIGO ###\n",
    "    clogs = None\n",
    "    cost = None\n",
    "    ### FIN DE TU CDIGO ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholder(64, 64, 3, 6)\n",
    "    parameters = init_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = cost_function(Z3, Y)\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    a = session.run(cost, {X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)})\n",
    "    print(\"cost = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Integraci贸n del modelo ConvNet\n",
    "\n",
    "Finalmente, integremos las funciones que implementamos anteriormente para construir un modelo. Posteriormente, entrenaremos el conjunto de datos `train_signs.h5` utilizando mini lotes. En el archivo `utils.py` se encuentra implementada la funci贸n `random_mini_batches()`. Recuerde que esta funci贸n devuelve una lista de mini lotes.\n",
    "\n",
    "El modelo deber铆a:\n",
    "\n",
    "- Crear placeholders\n",
    "- Inicializar par谩metros\n",
    "- Realizar el forward propagation\n",
    "- Calcular el costo\n",
    "- Crear un optimizador\n",
    "\n",
    "Finalmente, crear谩 una sesi贸n y ejecutar谩 un bucle for para num_epochs, obtendr谩 los mini lotes y luego, para cada mini lote, optimizar谩 la funci贸n. [[doc para inicializaci贸n las variables]](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)\n",
    "\n",
    "Completemos la siguiente funci贸n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.006, num_epochs = 100, minibatch_size = 64):\n",
    "    \"\"\"\n",
    "    Recordemos la estructura de la ConvNet:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Par谩metros: \n",
    "    X_train          dataset de entrenamiento, dimensiones (None, 64, 64, 3)\n",
    "    Y_train          etiquetas reales del dataset de entrenamiento, dimensiones (None, n_y = 6)\n",
    "    X_test           dataset de pruebas, dimensiones (None, 64, 64, 3)\n",
    "    Y_test           etiquetas reales del dataset de pruebas, dimensiones (None, n_y = 6)\n",
    "    learning_rate    tasa de aprendizaje del optimizador\n",
    "    num_epochs       n煤mero de epocas del ciclo del optimizador\n",
    "    minibatch_size   tama帽o del mini batch \n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                   # permite ejecutar el modelo sin sobre escribir las variables tf\n",
    "    tf.set_random_seed(1)                       # para mantener los resultados consistentes tensorflow\n",
    "    seed = 3                                    # para mantener los resultados consistentes en numpy \n",
    "    \n",
    "    (m, n_H0, n_W0, n_C0) = None       # identificar las dimensiones del dataset de entrenamiento\n",
    "    n_y = None                      # identificar el n煤mero de etiquetas\n",
    "    costs = []                                  # Para mantener un registro de los costos\n",
    "    \n",
    "    # Crear placeholders con las dimensiones correctas\n",
    "    ### INICIA TU CDIGO ### \n",
    "    X, Y = None\n",
    "    ### FINALIZA TU CDIGO ###\n",
    "\n",
    "    # Inicializar los par谩metros\n",
    "    ### INICIA TU CDIGO ### (1 line)\n",
    "    parameters = None\n",
    "    ### FINALIZA TU CDIGO ###\n",
    "    \n",
    "    # Forward propagation: crear el forward propagation en el grafo de c贸mputo de TensorFlow\n",
    "    ### INICIA TU CDIGO ### (1 line)\n",
    "    Z3 = None\n",
    "    ### FINALIZA TU CDIGO ###\n",
    "    \n",
    "    # Funci贸n de costo: agrega la funci贸n de costo al grafo de c贸mputo\n",
    "    ### INICIA TU CDIGO ### (1 line)\n",
    "    cost = None\n",
    "    ### FINALIZA TU CDIGO ###\n",
    "    \n",
    "    # Backpropagation: definir el optimizador en TensorFlow. Utilicemos un optimizador AdamOptimizer\n",
    "    # para minimizar el costo\n",
    "    \n",
    "    ### INICIA TU CDIGO ### (1 line)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    ### FINALIZA TU CDIGO ###\n",
    "    \n",
    "    # Inicializar globalmente todas las variables\n",
    "    init = None\n",
    "     \n",
    "    # Iniciar la sesi贸n para evaluar el grafo de TensorFlow\n",
    "    with tf.Session() as session:\n",
    "        \n",
    "        # Ejecutar la inicializaci贸n\n",
    "        session.run(None)\n",
    "        \n",
    "        # Ciclo de entrenamiento\n",
    "        for epoch in range(num_epochs):\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed) #Definida en utils.py\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                # Selecciona un minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: Ejecuci贸n del grafo sobre un minibatch.\n",
    "                # Ejecutar la sesi贸n para ejecutar el optimizador y la funci贸n de costo, t\n",
    "                # el feed_dict debe contener un minibatch (X,Y).\n",
    "                ### INICIA TU CDIGO ### \n",
    "                _ , temp_cost = session.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "                ### FINALIZA TU CDIGO ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "            # Imprimir el costo\n",
    "            if epoch % 10 == 0:\n",
    "                print (f\"Costo despues de la epoca {epoch}: {None}\")\n",
    "            if epoch % 1 == 0:\n",
    "                costs.append(None)\n",
    "        \n",
    "        \n",
    "        # graficar el costo\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('Costo')\n",
    "        plt.xlabel('Epocas')\n",
    "        plt.title(\"Tasa de aprendizaje:\" + str(None))\n",
    "        plt.show()\n",
    "\n",
    "        # Calcular las predicciones correctas\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calcular la exactitud sobre el conjunto de pruebas\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Exactitud (Training dataset):\", train_accuracy)\n",
    "        print(\"Exactitud (Test datset):\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
