{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex05 Introducción a TensorFlow-GPU\n",
    "\n",
    "Hasta ahora, hemos utilizado numpy para construir redes neuronales. Ahora comenzaremos a utilizar TensorFlow como framework para construir redes neuronales profundas de una forma más rápida. Los frameworks como TensorFlow, Caffe y Keras pueden acelerar significativamente el desarrollo de modelos de aprendizaje automático. En esta actividad, aprenderemos realizar los siguiente en TensorFlow:\n",
    "\n",
    "- Uso de elementos básicos (constantes, variables, sesiones, placeholders)\n",
    "- Algoritmos de entrenamiento\n",
    "- Implementar una red neuronal\n",
    "\n",
    "$\\textbf{Nota}$: los frameworks puede reducir el tiempo de codificación, pero también pueden tener optimizaciones que mejoren la velocidad de ejecución del código.\n",
    "\n",
    "$\\textbf{Observación}$: esta actividad ha sido diseñada para TensorFlow r2.0 para GPU, si utliza otra versión podría encontrar algunos detalles de sintaxis o en el uso de funciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Elementos básicos de TensorFlow\n",
    "\n",
    "### 1.1 Uso de liberías\n",
    "\n",
    "Comencemos por importar la librería `tensorflow` como `tf`. En caso de que aun no tenga instalado TensorFlow, se puede optar por las siguientes opciones:\n",
    "\n",
    "- [Utilizar un docker](https://www.tensorflow.org/install#run-a-tensorflow-container).\n",
    "- [Utilizar Google colab](https://www.tensorflow.org/install#google-colab58-an-easy-way-to-learn-and-use-tensorflow)\n",
    "\n",
    "\n",
    "Adicionalmente, para esta actividad vamos a necesitar la librería `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow_core._api.v2.version' from 'c:\\\\_devtools\\\\python\\\\anaconda3\\\\envs\\\\venvgpujupyter\\\\lib\\\\site-packages\\\\tensorflow_core\\\\_api\\\\v2\\\\version\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# %matplotlib inline   This line works only in colab\n",
    "\n",
    "print(tf.version)\n",
    "# Con el fin de poder comparar los resultados\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. 2 Uso de constantes, variables, sesiones y placeholders tensorflow 2.0\n",
    "\n",
    "TensorFlow 1.X requires users to manually stitch together an abstract syntax tree (the graph) by making tf.* API calls. It then requires users to manually compile the abstract syntax tree by passing a set of output tensors and input tensors to a session.run() call. TensorFlow 2.0 executes eagerly (like Python normally does) and in 2.0, graphs and sessions should feel like implementation details(within a tf.function, code with side effects execute in the order written).\n",
    "\n",
    "TensorFlow 1.X relied heavily on implicitly global namespaces.You could then recover that tf.Variable, but only if you knew the name that it had been created with.Variable scopes, global collections, helper methods like tf.get_global_step(), tf.global_variables_initializer() were used to make the developer know the name of the variable.\n",
    "\n",
    "!!!TensorFlow 2.0 eliminates all of these mechanisms in favor of the default mechanism: Keep track of your variables! If you lose track of a tf.Variable, it gets garbage collected.\n",
    "\n",
    "The requirement to track variables creates some extra work for the user, but with Keras objects (see below), the burden is minimized.\n",
    "\n",
    "#### Functions, not sessions\n",
    "\n",
    "#### TensorFlow 1.X\n",
    "outputs = session.run(f(placeholder), feed_dict={placeholder: input})\n",
    "#### TensorFlow 2.0\n",
    "outputs = f(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 3 Uso de constantes, variables, sesiones y placeholders\n",
    "\n",
    "Ahora que hemos importado las librerías, comencemos con un ejercicio sencillo para implementar en Tensorflow. Escribamos el código que permita realizar el cálculo del error en la predicción de la salida $\\hat{Y}$ para un ejemplo de entrenamiento con respecto a la salida esperada $Y$. Esto es:\n",
    "\n",
    "$loss = \\mathcal{L}(\\hat{y}, y) = (\\hat y^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Para este ejercicio:\n",
    "- Definamos `y_hat = 25` y `y=29` como constantes. Podemos definir constantes en tensorflow de la siguiente manera `nom_constante = tf.constant(valor, dtype)`. Como se explico en TF 2.X ya no es importante el nombre de la variable pero olvidemos el typo de dato.\n",
    "\n",
    "- Definamos la variable `loss`que debe almacenar el resultado del cálculo. Podemos definir variables en tensorflow de la siguiente manera `nom_variable = tf.Variable(valor, dtype)`\n",
    "\n",
    "- Inicialicemos las variables. No es requerido.\n",
    "\n",
    "- Ejecutemos la inicialización y el cálculo de `loss`. Se ejecutara automaticamente cuando se use.\n",
    "\n",
    "Para profundizar en el tema de uso de constantes, variables y sesiones se recomienda consultar las siguientes los siguiente enlaces:\n",
    "\n",
    "- [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable)\n",
    "- [tf.constant](https://www.tensorflow.org/api_docs/python/tf/constant)\n",
    "- [Tensorflow 2.0](https://www.tensorflow.org/guide/effective_tf2)\n",
    "\n",
    "Complete el código faltante en la siguiente celda, ejecute y verifique que el resultado concuerda con la salida esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "(2, 3)\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=int32, numpy=\n",
      "array([[1, 2, 3],\n",
      "       [4, 5, 6]])>\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]], shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "## Scalars\n",
    "string = tf.Variable(\"this is a string\", tf.string)\n",
    "number = tf.Variable(324, tf.int16)\n",
    "floatVar = tf.Variable(4.99, tf.float64)\n",
    "\n",
    "## Vectors\n",
    "vectorStr = tf.Variable([\"one\",\"two\",\"three\"], tf.string)\n",
    "vectorInt = tf.Variable([1,2,3], tf.int16)\n",
    "\n",
    "## Matrix\n",
    "vectorInt2 = tf.Variable([[1,2,3], [4,5,6]], tf.int16)\n",
    "\n",
    "# Lets check the dimentions\n",
    "print(tf.rank(string))\n",
    "print(tf.rank(vectorStr))\n",
    "print(tf.rank(vectorInt2))\n",
    "\n",
    "#shapes/reshape\n",
    "print(vectorInt2.shape)\n",
    "print(vectorInt2)\n",
    "reshapeVec = tf.reshape(vectorInt2, [3,2])\n",
    "reshapeVec = tf.reshape(vectorInt2, [3,-1])  # with -1 TF calculates the value\n",
    "print(reshapeVec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =\n",
      "16\r\n"
     ]
    }
   ],
   "source": [
    "y_hat = tf.constant(29, name = 'y_hat')                    #Definir una constante y_hat = 25\n",
    "y = tf.constant(25, name = 'y')                            #Definir una constante y = 29\n",
    "loss = tf.Variable((y-y_hat)**2, name='loss')              #Definir una variable y asignar (y-y_hat)**2\n",
    "print(\"loss =\")\n",
    "tf.print(loss)                                             #Ejecutar el cálculo de loss al imprimir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Salida esperada: }$\n",
    "\n",
    "`loss = 16`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que escribir y ejecutar programas en TensorFlow requiere seguir los siguientes pasos:\n",
    "\n",
    "1. Crear Tensors (por ejemplo, constantes y variables) que serán ejecutas/evaluadas posteriormente.\n",
    "2. Definir operaciones entre los Tensores.\n",
    "5. Ejecutar\n",
    "\n",
    "Por eso, cuando creamos la variable para el error, simplemente se define $loss$ en función de otros elementos, pero no se evalua su valor. Para evaluarla, primero tenemos que ejecutar.\n",
    "\n",
    "Veamos otro ejemplo sencillo. En este caso, escribamos un programa con TensorFlow que realice la multiplicación de dos constantes. `a = 2` y `b = 10`. Almacene el resultado en una variable llamada `c` y al final imprima el resultado.\n",
    "\n",
    "Tip:\n",
    "- Para realizar la multiplicación en TensorFlow, podemos utilizar `tf.multiply(   ,   )`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 20\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2, name = 'a')\n",
    "b = tf.constant(10, name = 'b')\n",
    "c =  tf.multiply(a,b) \n",
    "print(f\"c = {c}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Salida esperada:}$\n",
    "\n",
    "`c = Tensor(\"Mul:0\", shape=(), dtype=int32)`\n",
    "\n",
    "`c = 20`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que la primer instrucción `print` no imprime el valor de la variable `c` (aun no ha sido evaluada). En resumen, recuerde inicializar sus variables, crear su sesión y ejecutar las operaciones dentro de la sesión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los placeholders no existen en TF 2.X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 9\n"
     ]
    }
   ],
   "source": [
    "z = tf.constant(3, name = 'z')\n",
    "r = 3*(z)\n",
    "print(f\"r = {r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, cuando definimos `z` no se tiene que especificar su valor. Un `placeholder` es simplemente una variable a la que asignaras un valor posteriormente, cuando se ejecute `Session`. Normalmente se alimentan los datos al `placeholder` mediante un `feed_dictionary` en el momento que se ejecuta `Session`.\n",
    "\n",
    "En resumen, el uso de `placeholders` permite a TensorFlow especificar las operaciones necesarias para un cálculo, es decir, construir un gráfo de cálculo pero en donde los valores se especificarán más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Algunos problemas para calentar motores\n",
    "\n",
    "### 2.1. Implementación de una función lineal\n",
    "\n",
    "Implemente una función (método si prefiere POO) que utilice TensorFlow para calcular la siguiente ecuación: $Y = WX + b$, donde $W$ y $X$ son matrices aleatorias y $b$ es un vector aleatorio.\n",
    "\n",
    "Para este ejercicio, considere las siguientes dimensiones: $W$:(4, 3), $X$: (3, 1), y $b$:(4, 1). A manera de ejemplo, observe la definición de la constante $X$ que tiene la forma (3, 1):\n",
    "```python\n",
    "X = tf.constant(np.random.randn(3,1), name = \"X\")\n",
    "```\n",
    "\n",
    "$\\textbf{Nota}$: pueden resultar útil el uso de las siguientes funciones:\n",
    "- `tf.mul(   ,   )` para realizar la multiplicación de dos matrices\n",
    "- `tf.add(   ,   )` para realizar la suma\n",
    "- `tf.random.randn(  )` para inicializar de manera aleatoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result = [[-2.15657382]\n",
      " [ 2.95891446]\n",
      " [-1.08926781]\n",
      " [-0.84538042]]\n"
     ]
    }
   ],
   "source": [
    "def linear_function():\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    #Inicialice las constantes X, W, b\n",
    "    X = tf.constant(np.random.randn(3,1), name = \"X\")\n",
    "    W = tf.constant(np.random.randn(4,3), name = \"W\")\n",
    "    b = tf.constant(np.random.randn(4,1), name = \"b\")\n",
    "\n",
    "    #Defina las operaciones del grafo de cómputo\n",
    "    Y = tf.add(tf.matmul(W,X), b)\n",
    "    \n",
    "    #Crear la sesión y ejecutarla\n",
    "    \n",
    "    return Y\n",
    "\n",
    "print(f\"result = {linear_function()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Salida esperada: }$\n",
    "\n",
    "`result = [[-2.15657382]`\n",
    "\n",
    "` [ 2.95891446]`\n",
    "\n",
    "` [-1.08926781]`\n",
    "\n",
    "` [-0.84538042]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Cálculo de la sigmoidal\n",
    "\n",
    "TensorFlow provee un variedad de funciones comúnmente utilizadas para redes neuronales como:\n",
    "- `tf.sigmoid()`\n",
    "- `tf.softmax()`\n",
    "\n",
    "Para este ejercicio, implemente una función para calcular la función sigmoidal de una entrada. Para implementar el ejercico, utilicemos el `placeholder` $x$. Cuando ejecutemos la sesión, utilicemos un `feed dictionary` para asignar el valor de entrada $z$. En resumen, tenemos que seguir los siguientes pasos:\n",
    "\n",
    "- Crear un `placeholder` llamado x\n",
    "- Definir las operaciones necesarias para calcular la función sigmoidal\n",
    "- Ejecutar la sesión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n",
      "sigmoid(12) = 0.9999938558253978\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    # Crear tensor\n",
    "    X = tf.constant(z, tf.float64)\n",
    "    \n",
    "    # Definir el cálculo de la sigmoidal\n",
    "    result = tf.sigmoid(X)\n",
    "    return result\n",
    "\n",
    "print(f\"sigmoid(0) = {sigmoid(0)}\")\n",
    "print(f\"sigmoid(12) = {sigmoid(12)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Salida esperada:}$\n",
    "\n",
    "`sigmoid(0) = 0.5`\n",
    "\n",
    "`sigmoid(12) = 0.9999938011169434`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Calcular la función de costo\n",
    "\n",
    "Anteriormente implementamos la función de costo desde cero: \n",
    "\n",
    "$J = - \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{[2](i)}) + (1-y^{(i)})\\log (1-\\sigma(z^{[2](i)})\\large )\\small$\n",
    "\n",
    "sin embargo, en TensorFlow no es necesario.\n",
    "\n",
    "En este ejercicio, implementemos una función que calcule el costo, para esto utilizaremos la función: `tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)`\n",
    "\n",
    "Tu implementación debe recibir `logits` (nuestros valores `z`), calcular la sigmoidal para obtener la activación y entonces calcular la función de costo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = [1.00538722 1.03664083 0.41385432 0.39956614]\n"
     ]
    }
   ],
   "source": [
    "def cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Parámetros:\n",
    "    \n",
    "    logits  vector que contiene las entradas a la unidad de activación (antes de la activación sigmoidal final)\n",
    "    labels  vector de etiquetas (1 o 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Crear tensors\n",
    "    z = tf.constant(logits, tf.float64)\n",
    "    y = tf.constant(labels, tf.float64)\n",
    "    \n",
    "    # Utilice la función de costo\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = z,  labels = y)\n",
    "    \n",
    "    \n",
    "    return cost\n",
    "\n",
    "logits = sigmoid(np.array([0.2,0.4,0.7,0.9]))\n",
    "cost = cost(logits, np.array([0,0,1,1]))\n",
    "print (f\"cost = {cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Salida esperada:}$\n",
    "\n",
    "`cost = [1.0053872  1.0366409  0.41385433 0.39956614]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Representación One Hot\n",
    "\n",
    "Muchas veces en aprendizaje profundo tendremos un vector con números que van desde 0 a $C$-1, donde $C$ es el número de clases. Si $C$ es 4, por ejemplo, entonces podría tener el siguiente vector y que deberá convertir de la siguiente manera:\n",
    "\n",
    "<img src=\"images/one_hot.png\" style=\"width:600px;height:150px;\">\n",
    "\n",
    "A esto se le llama representación `One Hot`, porque en la nueva representación únicamente un elemento por columna se encuentra encendido (valor a 1). La implementación de esta transformación puede requerir alguna lineas de código en `numpy`, sin embargo, en TensorFlow podemos utilizar la siguiente función:\n",
    "\n",
    "- tf.one_hot(labels, depth, axis) \n",
    "\n",
    "Implementemos una función que reciba un vector de etiquetas y el número de clases. La función debe retornar una representación \"One Hot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoding(label, C):\n",
    "    C = tf.constant(C, tf.int32)\n",
    "    \n",
    "    one_hot_encode = tf.one_hot(labels, C, axis=0)\n",
    "       \n",
    "    return one_hot_encode\n",
    "\n",
    "labels = np.array([1,2,0,2,1,0])\n",
    "one_hot = one_hot_encoding(labels, C = 3)\n",
    "print (f\"{one_hot}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Salida esperada:}$\n",
    "\n",
    "`[[0. 0. 1. 0. 0. 1.]`\n",
    "\n",
    "` [1. 0. 0. 0. 1. 0.]`\n",
    "\n",
    "` [0. 1. 0. 1. 0. 0.]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Inicializar un vector con ceros y unos\n",
    "\n",
    "Para inicializar un vector con ceros y unos. La función que utilizaremos es `tf.ones ()`. Para inicializar con ceros, puede usar `tf.zeros ()` en su lugar. Estas funciones toma un `shape` y retorna una matriz con la misma dimensiones pero llena de ceros y unos, respectivamente.\n",
    "\n",
    "Implementemos la siguiente función para basados en un `shape`, devolver una matriz (de la misma dimensión con unos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def ones(shape):\n",
    "    \n",
    "    ones = tf.ones(shape)\n",
    "    \n",
    "    return ones\n",
    "\n",
    "print (f\"{ones([3])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Salida esperada:}$\n",
    "\n",
    "`[1. 1. 1.]`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
